{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Sparse Data and Embeddings\n",
    "\n",
    "In this Datalab exercise, we'll explore sparse data and work with embeddings using text data from movie reviews (from the [ACL 2011 IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)). This data has already been processed into `tf.Example` format.  \n",
    "\n",
    "Let's download the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/test.tfrecord -O /tmp/test.tfrecord\n",
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/train.tfrecord -O /tmp/train.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a sentiment-analysis model on this data that predicts if a review is generally *favorable* (label of 1) or *unfavorable* (label of 0).\n",
    "\n",
    "To do so, we'll turn our string-value `terms` into feature vectors by using a *vocabulary*, a list of each term we expect to see in our data. For the purposes of this exercise, we've created a small vocabulary that focuses on a limited set of terms. Most of these terms were found to be strongly indicative of *favorable* or *unfavorable*, but some were just added because they're interesting.\n",
    "\n",
    "Each term in the vocabulary is mapped to a coordinate in our feature vector. To convert the string-value `terms` for an example into this vector format, we encode such that each coordinate gets a value of 0 if the vocabulary term does not appear in the example string, and a value of 1 if it does. Terms in an example that don't appear in the vocabulary are thrown away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *We could of course use a larger vocabulary, and there are special tools for creating these. In addition, instead of just dropping terms that are not in the vocabulary, we can introduce a small number of OOV (out-of-vocabulary) buckets to which you can hash the terms not in the vocabulary. We can also use a __feature hashing__ approach that hashes each term, instead of creating an explicit vocabulary. This works well in practice, but loses interpretability, which is useful for this exercise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Use a linear model with sparse inputs and an explicit vocabulary\n",
    "\n",
    "First, we'll build a [`LinearClassifier`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearClassifier) model using 54 informative terms; always start simple!\n",
    "\n",
    "The [`sparse_column_with_keys`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/sparse_column_with_keys) function creates a feature column with the string-to-feature-vector mapping.\n",
    "\n",
    "After you read through the code, run it and see how you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split them into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=25,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "# 54 informative terms that compose our model vocabulary \n",
    "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                       \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                       \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\" ]\n",
    "\n",
    "# Create a feature column from \"terms\", using our informative terms.\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "feature_columns = [ terms_feature_column ]\n",
    "\n",
    "classifier = tf.contrib.learn.LinearClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  optimizer=tf.train.AdagradOptimizer(\n",
    "    learning_rate=0.1),\n",
    "  gradient_clip_norm=5.0\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use a Deep Neural Network (DNN) model\n",
    "\n",
    "The above model is a linear model.  It works quite well.  But can we do better with a DNN model?  Let's try.\n",
    "\n",
    "Let's swap in a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier) for the `LinearClassifier`. Run the following cell.  What kind of errors do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=25,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                       \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                       \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\" ]\n",
    "\n",
    "# Create a feature column from \"terms\", using feature hashing.\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "feature_columns = [ terms_feature_column ]\n",
    "\n",
    "##################### Here's what we changed ##################################\n",
    "classifier = tf.contrib.learn.DNNClassifier(                                  #\n",
    "  feature_columns=feature_columns,                                            #\n",
    "  hidden_units=[20,20],                                                       #\n",
    "  optimizer=tf.train.AdagradOptimizer(                                        #\n",
    "    learning_rate=0.1),                                                       #\n",
    "  gradient_clip_norm=5.0                                                      #\n",
    ")                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "try:\n",
    "  classifier.fit(\n",
    "    input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "    steps=1000)\n",
    "\n",
    "  evaluation_metrics = classifier.evaluate(\n",
    "    input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "    steps=1)\n",
    "  print \"Training set metrics:\"\n",
    "  for m in evaluation_metrics:\n",
    "    print m, evaluation_metrics[m]\n",
    "  print \"---\"\n",
    "\n",
    "  evaluation_metrics = classifier.evaluate(\n",
    "    input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "    steps=1)\n",
    "\n",
    "  print \"Test set metrics:\"\n",
    "  for m in evaluation_metrics:\n",
    "    print m, evaluation_metrics[m]\n",
    "  print \"---\"\n",
    "except ValueError as err:\n",
    "  print err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Use an embedding with a DNN model\n",
    "\n",
    "Training a DNNClassifier with a sparse feature column (e.g., `_SparseColumnKeys`) results in an error. We need some method to pass sparse input data to a `DNNClassifier`. As we saw in the error output for **Task 2**, we have two options: an embedding column or a one-hot column.\n",
    "\n",
    "In this task, we'll implement our DNN model using an embedding column. An embedding column takes sparse data as input and returns a lower-dimensional dense vector as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *An embedding_column is usually the computationally most efficient option to use for training a model on sparse data. In an [optional section](#scrollTo=XDMlGgRfKSVz) at the end of this Datalab, we'll discuss in more depth the implementational differences between using an `embedding_column` and a `one_hot_column`, and the tradeoffs of selecting one over the other.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, do the following:\n",
    "\n",
    "* Define the feature columns for the model using an `embedding_column` that projects the data into 2 dimensions (see the [TF docs](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column) for more details on the function signature for `embedding_column`).\n",
    "* Define a `DNNClassifier` with the following specifications:\n",
    "  * Two hidden layers of 20 units each\n",
    "  * Adagrad optimization with a learning rate of 0.1\n",
    "  * A `gradient_clip_norm` of 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *In practice, we might project to dimensions higher than 2, like 50 or 100.  But for now, 2 dimensions is easy to visualize.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Hint:\n",
    "# Here's a example code snippet you might use to define the feature columns:\n",
    "\n",
    "terms_embedding_column = tf.contrib.layers.embedding_column(terms_feature_column, dimension=2)\n",
    "feature_columns = [ terms_embedding_column ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=25,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "\n",
    "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                       \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                       \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\" ]\n",
    "\n",
    "# Create a feature column from \"terms\", using feature hashing.\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "########################## YOUR CODE HERE ######################################\n",
    "terms_embedding_column = # Define the embedding column\n",
    "feature_columns = # Define the feature columns\n",
    "\n",
    "classifier = # Define the DNNClassifier\n",
    "################################################################################\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Convince yourself there's actually an embedding in there\n",
    "\n",
    "The above model used an `embedding_column`, and it seemed to work, but this doesn't tell us much about what's going on internally. How can we check that the model is actually using an embedding inside?\n",
    "\n",
    "To start, let's look at the tensors in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.get_variable_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we can see that there is an embedding layer in there: `'dnn/input_from_feature_columns/terms_embedding/...'`. (What's interesting here, by the way, is that this layer is trainable along with the rest of the model just as any hidden layer is.)\n",
    "\n",
    "Is the embedding layer the correct shape? Run the following code to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *Remember, in our case, the embedding is a matrix that allows us to project a 54-dimensional vector down to 2 dimensions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.get_variable_value('dnn/input_from_feature_columns/terms_embedding/weights').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time manually checking the various layers and shapes to make sure everything is connected the way you would expect it would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Examine the embedding\n",
    "\n",
    "Let's now take a look at the actual embedding space, and see where the terms end up in it. Do the following:\n",
    "1. Run the following code to see the embedding we trained in **Task 3**. Do things end up where you'd expect?\n",
    "\n",
    "2. Re-train the model by rerunning the code in **Task 3**, and then run the embedding visualization below again. What stays the same? What changes?\n",
    "\n",
    "3. Finally, re-train the model again using only 10 steps (which will yield a terrible model). Run the embedding visualization below again. What do you see now, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "embedding_matrix = classifier.get_variable_value('dnn/input_from_feature_columns/terms_embedding/weights')\n",
    "\n",
    "for term_index in range(len(informative_terms)):\n",
    "  # Create a one-hot encoding for our term.  It has 0's everywhere, except for\n",
    "  # a single 1 in the coordinate that corresponds to that term.\n",
    "  term_vector = np.zeros(len(informative_terms))\n",
    "  term_vector[term_index] = 1\n",
    "  # We'll now project that one-hot vector into the embedding space.\n",
    "  embedding_xy = np.matmul(term_vector, embedding_matrix)\n",
    "  plt.text(embedding_xy[0],\n",
    "           embedding_xy[1],\n",
    "           informative_terms[term_index])\n",
    "\n",
    "# Do a little set-up to make sure the plot displays nicely.\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "plt.xlim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
    "plt.ylim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6:  Try to improve the model's performance\n",
    "\n",
    "See if you can refine the model to improve performance. A couple things you may want to try:\n",
    "\n",
    "* **Changing hyperparameters**, or **using a different optimizer** like Adam (you may only gain one or two accuracy percentage points following these strategies).\n",
    "* **Adding additional terms to `informative_terms`.** There's a full vocabulary file with all 30716 terms for this data set that you can use at: https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/terms.txt You can pick out additional terms from this vocabulary file, or use the whole thing via the `sparse_column_with_vocabulary_file` feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/terms.txt -O /tmp/terms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=100,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "# Create a feature column from \"terms\", using a full vocabulary file.\n",
    "informative_terms = None\n",
    "with open(\"/tmp/terms.txt\", 'r') as f:\n",
    "  # Convert it to set first to remove duplicates.\n",
    "  informative_terms = list(set(f.read().split()))\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "terms_embedding_column = tf.contrib.layers.embedding_column(terms_feature_column, dimension=2)\n",
    "feature_columns = [ terms_embedding_column ]\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=[10, 10],\n",
    "  optimizer=tf.train.AdamOptimizer(\n",
    "    learning_rate=0.001),\n",
    "  gradient_clip_norm=1.0\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final word\n",
    "\n",
    "We may have gotten a DNN solution with an embedding that was better than our original linear model, but the linear model was also pretty good and was quite a bit faster to train. Linear models train more quickly because they do not have nearly as many parameters to update or layers to backprop through.\n",
    "\n",
    "In some applications, the speed of linear models may be a game changer, or linear models may be perfectly sufficient from a quality standpoint. In other areas, the additional model complexity and capacity provided by DNNs might be more important. When defining your model architecture, remember to explore your problem sufficiently so that you know which space you're in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Optional Discussion:* Trade-offs between `embedding_column` and `one_hot_column`\n",
    "\n",
    "Conceptually when training a `LinearClassifier` or a `DNNClassifier`, there is an adapter needed to use a sparse column. TF provides two options: `embedding_column` or `one_hot_column`.\n",
    "\n",
    "When training a LinearClassifier (as in **Task 1**), an `embedding_column` in used under the hood. As seen in **Task 2**, when training a `DNNClassifier`, you must explicitly choose either `embedding_column` or `one_hot_column`. This section discusses the distinction between the two, and the trade-offs of using one over the other, by looking at a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have sparse data containing the values `\"great\"`, `\"beautiful\"`, `\"excellent\"`. Since the vocabulary size we're using here is $V = 54$, each unit (neuron) in the first layer will have 54 weights. We denote the number of terms in a sparse input using $s$. So for this example sparse data, $s = 3$. For an input layer with $V$ possible values, a hidden layer with $d$ units needs to do a vector-matrix multiply: $(1 \\times V) * (V \\times d)$.  This has $O(V * d)$ computational cost. Note that this cost is proportional to the number of weights in that hidden layer and independent of $s$.\n",
    "\n",
    "If the inputs are one-hot encoded (a Boolean vector of length $V$ with a 1 for the terms present and a 0 for the rest) using a [`one_hot_column`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/one_hot_column), this means multiplying and adding a lot of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we achieve the exact same results by using an [`embedding_column`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column) of size $d$, we look up and add up just the embeddings corresponding to the three features present in our example input of \"`great`\", \"`beautiful`\", \"`excellent`\": $(1 \\times d) + (1 \\times d) + (1 \\times d)$. Since the weights for the features that are absent are multiplied by zero in the vector-matrix multiply, they do not contribute to the result. Weights for the features that are present are multiplied by 1 in the vector-matrix multiply. Thus, adding the weights obtained via the embedding lookup will lead to the same result as in the vector-matrix-multiply.\n",
    "\n",
    "When using an embedding, computing the embedding lookup is an $O(s * d)$ computation, which is computationally much more efficient than the $O(V * d)$ cost for the `one_hot_column` in sparse data for which $s$ is much smaller than $V$. (Remember, these embeddings are being learned. In any given training iteration it is the current weights that are being looked up.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in **Task 3**, by using an `embedding_column` in training the `DNNClassifier`, our model learns a low-dimensional representation for the features, where the dot product defines a similarity metric tailored to the desired task. In this example, terms that are used similarly in the context of movie reviews (e.g., `\"great\"` and `\"excellent\"`) will be closer to each other the embedding space (i.e., have a large dot product), and terms that are dissimilar (e.g., `\"great\"` and `\"bad\"`) will be farther away from each other in the embedding space (i.e., have a small dot product)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
